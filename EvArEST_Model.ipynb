{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Text Recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image as pilImg\n",
    "import os \n",
    "import cv2\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import arabic_reshaper #Reshaping arabic texts\n",
    "from bidi.algorithm import get_display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import random\n",
    "from keras import backend as K\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Letters present in the Label Text\n",
    "# letters= '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('EvArEST/Arabic_words_train/gt.txt', delimiter=',', header=None, names=['image', 'text'])\n",
    "test_df = pd.read_csv('EvArEST/Arabic_words_test/gt.txt', delimiter=',', header=None, names=['image', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ''.join(set(''.join(train_df['text'])) | {'/'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image height\n",
    "img_h=32\n",
    "#image width\n",
    "img_w=170\n",
    "#image Channels\n",
    "img_c=1\n",
    "# classes for softmax with number of letters +1 for blank space in ctc\n",
    "num_classes=len(letters)+1\n",
    "batch_size=64\n",
    "max_length=15 # considering max length of ground truths labels to be 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words_labels(word):\n",
    "    \"\"\"\n",
    "    Encodes the Ground Truth Labels to a list of Values like eg.HAT returns [17,10,29]\n",
    "    \"\"\"\n",
    "    label_lst=[]\n",
    "    for char in word:\n",
    "        label_lst.append(letters.find(char)) # keeping 0 for blank and for padding labels\n",
    "    return label_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_from_labels(labels):\n",
    "    \"\"\"\n",
    "    converts the list of encoded integer labels to word strings like eg. [12,10,29] returns CAT \n",
    "    \"\"\"\n",
    "    txt=[]\n",
    "    for ele in labels:\n",
    "        if ele == len(letters): # CTC blank space\n",
    "            txt.append(\"\")\n",
    "        else:\n",
    "            #print(letters[ele])\n",
    "            txt.append(letters[ele])\n",
    "    return \"\".join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss_function(args):\n",
    "    \"\"\"\n",
    "    CTC loss function takes the values passed from the model returns the CTC loss using Keras Backend ctc_batch_cost function\n",
    "    \"\"\"\n",
    "    y_pred, y_true, input_length, label_length = args \n",
    "    # since the first couple outputs of the RNN tend to be garbage we need to discard them, found this from other CRNN approaches\n",
    "    # I Tried by including these outputs but the results turned out to be very bad and got very low accuracies on prediction \n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(y_true, y_pred, input_length, label_length)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the CTC loss fuction computed using Keras Backend ctc_batch_cost function requires 4 inputs, we build a **Data genearator class** and define the parameters for our input image and also process the 4 inputs which needs to be passed to the model for computing CTC loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/qjadud1994/CRNN-Keras\n",
    "#https://keras.io/examples/image_ocr/\n",
    "\n",
    "class DataGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, img_dirpath, img_w, img_h,\n",
    "                 batch_size,n,output_labels,max_text_len=15):\n",
    "        self.img_h = img_h                    #Image Height\n",
    "        self.img_w = img_w                    #Image Width\n",
    "        self.batch_size = batch_size          #Batch size of Input\n",
    "        self.max_text_len = max_text_len      #Maximum Text length of Labels\n",
    "        \n",
    "#         self.n =len(self.img_dir)                           #Number of images in img_matrix\n",
    "        self.n=n\n",
    "        self.img_dir = img_dirpath[:self.n]     # images list\n",
    "        self.indexes = list(range(self.n))   #List of indices for each image in img_matrix\n",
    "        self.cur_index = 0                   #Current index which points to image being loaded \n",
    "        self.imgs = np.zeros((self.n, self.img_h, self.img_w))\n",
    "        self.texts =  output_labels[:self.n]                  #List of Ground Truth Label texts\n",
    "\n",
    "   \n",
    "    def build_data(self):\n",
    "        \"\"\"\n",
    "        Build The Image Data\n",
    "        \"\"\"\n",
    "        print(self.n, \" Image Loading start...\")\n",
    "        for i, img_file in enumerate(self.img_dir):\n",
    "            img = cv2.imread(img_file)\n",
    "            img = img[:,:,1]                               #Extracting Single Channel Image\n",
    "            img = cv2.resize(img, (self.img_w, self.img_h))\n",
    "            img = img /255\n",
    "            self.imgs[i, :, :]= img\n",
    "            if i%1000==0:\n",
    "                print(\"Loaded Images: \",i)\n",
    "           \n",
    "        print(\"Number of Texts matches with Total Number of Images :\",len(self.texts) == self.n)\n",
    "        print(self.n, \" Image Loading finish...\")\n",
    "\n",
    "\n",
    "    def next_data(self): \n",
    "        \"\"\"\n",
    "        Returns image and text data pointed by the current index\n",
    "        \"\"\"\n",
    "        self.cur_index += 1\n",
    "        #If current index becomes more than the number of images, make current index 0 \n",
    "        #and shuffle the indices list for random picking of image and text data\n",
    "        if self.cur_index >= self.n:\n",
    "            self.cur_index = 0\n",
    "            random.shuffle(self.indexes)\n",
    "        return self.imgs[self.indexes[self.cur_index]], self.texts[self.indexes[self.cur_index]]\n",
    "\n",
    "    def next_batch(self):\n",
    "        \"\"\"\n",
    "        Creates a batch of images images and text data equal to the batch_size,\n",
    "        computes the parameters needed for CTC and returns the inputs to the Model\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            X_data = np.ones([self.batch_size, self.img_w, self.img_h, 1])  #Single channel Gray Size Scale images for input\n",
    "            #Initilizing with -1 to aid for padding labels of different lengths\n",
    "            Y_data = np.ones([self.batch_size, self.max_text_len])* -1        #Text labels for input\n",
    "           #input_length for CTC which is the number of time-steps of the RNN output\n",
    "            input_length = np.ones((self.batch_size, 1)) * 40\n",
    "            label_length = np.zeros((self.batch_size, 1))                   #label length for CTC\n",
    "            source_str=[]                                                   #List to store Ground Truth Labels\n",
    "            for i in range(self.batch_size):\n",
    "                img, text = self.next_data() #getting the image and text data pointed by current index\n",
    "                                    #taking transpose of image\n",
    "                img=img.T\n",
    "                img = np.expand_dims(img, -1)  #expanding image to have a single channel\n",
    "                X_data[i] = img\n",
    "                label=encode_words_labels(text) # encoding label text to integer list and storing in temp label variable\n",
    "                lbl_len=len(label)\n",
    "                Y_data[i,0:lbl_len] = label #Storing the label till its length and padding others\n",
    "                label_length[i] = len(label)\n",
    "                source_str.append(text) #storing Ground Truth Labels which will be accessed as reference for calculating metrics\n",
    "            \n",
    "        #Preparing the input for the Model\n",
    "            inputs = {\n",
    "                'img_input': X_data,  \n",
    "                'ground_truth_labels': Y_data,  \n",
    "                'input_length': input_length,  \n",
    "                'label_length': label_length,\n",
    "                'source_str': source_str  # used for visualization only\n",
    "            }\n",
    "            #Preparing output for the Model and intializing to zeros\n",
    "            outputs = {'ctc': np.zeros([self.batch_size])}  \n",
    "            yield (inputs, outputs) # Return the Prepared input and output to the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Overview of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below is the Representation of Model Overview presented in the Research Paper**\n",
    "\n",
    "<img src=\"Figs/model_overview.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The architecture consists of three parts:**\n",
    "1. convolutional layers, which extract a feature sequence from the input image; \n",
    "2. recurrentlayers, which predict a label distribution for each frame; \n",
    "3. transcription layer, which translates the per-frame predictions into the final label sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source: https://ieeexplore.ieee.org/abstract/document/7801919**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below is the Model Architecture presented in the Research Paper which is implemented as a part of the Case Study for Scene Text Recognition Problem** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source: https://ieeexplore.ieee.org/abstract/document/7801919**\n",
    "\n",
    "<img src=\"Figs/model_architecture.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, MaxPool2D, Dense,MaxPooling2D\n",
    "from keras.layers import AveragePooling2D, Flatten, Activation, Bidirectional\n",
    "from keras.layers import BatchNormalization, Dropout\n",
    "from keras.layers import Concatenate, Add, Multiply, Lambda\n",
    "from keras.layers import UpSampling2D, Reshape\n",
    "from keras.layers import add,concatenate\n",
    "from keras.layers import Reshape\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM,GRU\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1. Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model with Bi-Directional LSTM units and Adam Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Model Architecture has 2 Stages:\n",
    "1. **Train Stage:** which takes input image, text labels (encoded as integers), input length (time-steps length), label length and outputs CTC loss\n",
    "2. **Prediction Stage:** which takes input image and outputs a matrix of dimesnions 48x37 where 48 is the number of time-steps of RNN and 37 is the length of letters + 1 character for ctc blank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Image_text_recogniser_model_1(stage,drop_out_rate=0.35):\n",
    "    \"\"\"\n",
    "    Builds the model by taking in the stage variable which specifes the stage,\n",
    "    if the stage is training: model takes inputs required for computing ctc_batch_cost function\n",
    "    else : model takes input as images which is used for prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (1, img_w, img_h)\n",
    "    else:\n",
    "        input_shape = (img_w, img_h, 1)\n",
    "       \n",
    "    model_input=Input(shape=input_shape,name='img_input',dtype='float32')\n",
    "\n",
    "    # Convolution layer \n",
    "    model = Conv2D(64, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(model_input) \n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = MaxPooling2D(pool_size=(2, 2), name='max1')(model) \n",
    "\n",
    "    model = Conv2D(128, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(model) \n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = MaxPooling2D(pool_size=(2, 2), name='max2')(model) \n",
    "\n",
    "    model = Conv2D(256, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(model) \n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(256, (3, 3), padding='same', name='conv4', kernel_initializer='he_normal')(model)\n",
    "    model=Dropout(drop_out_rate)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = MaxPooling2D(pool_size=(1, 2), name='max3')(model)  \n",
    "\n",
    "    model = Conv2D(512, (3, 3), padding='same', name='conv5', kernel_initializer='he_normal')(model) \n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(512, (3, 3), padding='same', name='conv6')(model)\n",
    "    model=Dropout(drop_out_rate)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = MaxPooling2D(pool_size=(1, 2), name='max4')(model) \n",
    "\n",
    "    model = Conv2D(512, (2, 2), padding='same', kernel_initializer='he_normal', name='con7')(model)\n",
    "    model=Dropout(0.25)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)    \n",
    "\n",
    "    # CNN to RNN\n",
    "    model = Reshape(target_shape=((42, 1024)), name='reshape')(model)  \n",
    "#     model = tf.keras.Model(inputs=inputs, outputs=tf.reshape(inputs, [-1, 42, 1024]))\n",
    "    model = Dense(64, activation='relu', kernel_initializer='he_normal', name='dense1')(model)  \n",
    "\n",
    "    # RNN layer\n",
    "    model=Bidirectional(LSTM(256, return_sequences=True, kernel_initializer='he_normal'), merge_mode='sum')(model)\n",
    "    model=Bidirectional(LSTM(256, return_sequences=True, kernel_initializer='he_normal'), merge_mode='concat')(model)\n",
    "\n",
    "    # transforms RNN output to character activations:\n",
    "    model = Dense(num_classes, kernel_initializer='he_normal',name='dense2')(model) \n",
    "    y_pred = Activation('softmax', name='softmax')(model)\n",
    "\n",
    "    \n",
    "    labels = Input(name='ground_truth_labels', shape=[max_length], dtype='float32') \n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64') \n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64') \n",
    "\n",
    "    #CTC loss function\n",
    "    loss_out = Lambda(ctc_loss_function, output_shape=(1,),name='ctc')([y_pred, labels, input_length, label_length]) #(None, 1)\n",
    "\n",
    "    if stage=='train':\n",
    "        return model_input,y_pred,Model(inputs=[model_input, labels, input_length, label_length], outputs=loss_out)\n",
    "    else:\n",
    "        return Model(inputs=[model_input], outputs=y_pred)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input,y_pred,img_text_recog=Image_text_recogniser_model_1('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for visualization\n",
    "# it is a keras backend function used to capture the model ouputs so that it can be used for decoding and calculating metrics\n",
    "# test_func = K.function([model_input], [y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " img_input (InputLayer)         [(None, 170, 32, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv1 (Conv2D)                 (None, 170, 32, 64)  640         ['img_input[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 170, 32, 64)  256        ['conv1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 170, 32, 64)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " max1 (MaxPooling2D)            (None, 85, 16, 64)   0           ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)                 (None, 85, 16, 128)  73856       ['max1[0][0]']                   \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 85, 16, 128)  512        ['conv2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 85, 16, 128)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " max2 (MaxPooling2D)            (None, 42, 8, 128)   0           ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " conv3 (Conv2D)                 (None, 42, 8, 256)   295168      ['max2[0][0]']                   \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 42, 8, 256)  1024        ['conv3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 42, 8, 256)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv4 (Conv2D)                 (None, 42, 8, 256)   590080      ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 42, 8, 256)   0           ['conv4[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 42, 8, 256)  1024        ['dropout_3[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 42, 8, 256)   0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " max3 (MaxPooling2D)            (None, 42, 4, 256)   0           ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " conv5 (Conv2D)                 (None, 42, 4, 512)   1180160     ['max3[0][0]']                   \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 42, 4, 512)  2048        ['conv5[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 42, 4, 512)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv6 (Conv2D)                 (None, 42, 4, 512)   2359808     ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 42, 4, 512)   0           ['conv6[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 42, 4, 512)  2048        ['dropout_4[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 42, 4, 512)   0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " max4 (MaxPooling2D)            (None, 42, 2, 512)   0           ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " con7 (Conv2D)                  (None, 42, 2, 512)   1049088     ['max4[0][0]']                   \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 42, 2, 512)   0           ['con7[0][0]']                   \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 42, 2, 512)  2048        ['dropout_5[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 42, 2, 512)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 42, 1024)     0           ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " dense1 (Dense)                 (None, 42, 64)       65600       ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 42, 256)     657408      ['dense1[0][0]']                 \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_3 (Bidirectional  (None, 42, 512)     1050624     ['bidirectional_2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense2 (Dense)                 (None, 42, 78)       40014       ['bidirectional_3[0][0]']        \n",
      "                                                                                                  \n",
      " softmax (Activation)           (None, 42, 78)       0           ['dense2[0][0]']                 \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ground_truth_labels (InputLaye  [(None, 15)]        0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " input_length (InputLayer)      [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " label_length (InputLayer)      [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " ctc (Lambda)                   (None, 1)            0           ['softmax[0][0]',                \n",
      "                                                                  'ground_truth_labels[0][0]',    \n",
      "                                                                  'input_length[0][0]',           \n",
      "                                                                  'label_length[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,371,406\n",
      "Trainable params: 7,366,926\n",
      "Non-trainable params: 4,480\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_text_recog.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Function Takes the final 40x37 output matrices from the model for the batch from test_func function, and takes the argmax of the matrix across each column (which returns a value between 0 to 36 (ctc_blank) both included). The outputs are then merged for repeated values and gives a list of integers. The Final output integers is then converted to final output string text and stored in a list. The Final List containing all decoded outputs are returened by the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_batch(test_func, word_batch):\n",
    "    \"\"\"\n",
    "    Takes the Batch of Predictions and decodes the Predictions by Best Path Decoding and Returns the Output\n",
    "    \"\"\"\n",
    "    out = test_func([word_batch])[0] #returns the predicted output matrix of the model\n",
    "    ret = []\n",
    "    for j in range(out.shape[0]):\n",
    "        out_best = list(np.argmax(out[j, 2:], 1))\n",
    "        out_best = [k for k, g in itertools.groupby(out_best)]\n",
    "        outstr = words_from_labels(out_best)\n",
    "        ret.append(outstr)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracies(actual_labels,predicted_labels,is_train):\n",
    "    \"\"\"\n",
    "    Takes a List of Actual Outputs, predicted Outputs and returns their accuracy and letter accuracy across\n",
    "    all the labels in the list\n",
    "    \"\"\"\n",
    "    accuracy=0\n",
    "    letter_acc=0\n",
    "    letter_cnt=0\n",
    "    count=0\n",
    "    for i in range(len(actual_labels)):\n",
    "        predicted_output=predicted_labels[i]\n",
    "        actual_output=actual_labels[i]\n",
    "        count+=1\n",
    "        for j in range(min(len(predicted_output),len(actual_output))):\n",
    "            if predicted_output[j]==actual_output[j]:\n",
    "                letter_acc+=1\n",
    "        letter_cnt+=max(len(predicted_output),len(actual_output))\n",
    "        if actual_output==predicted_output:\n",
    "            accuracy+=1\n",
    "    final_accuracy=np.round((accuracy/len(actual_labels))*100,2)\n",
    "    final_letter_acc=np.round((letter_acc/letter_cnt)*100,2)\n",
    "    return final_accuracy,final_letter_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CallBacks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://keras.io/examples/image_ocr/\n",
    "class VizCallback(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    The Custom Callback created for printing the Accuracy and Letter Accuracy Metrics at the End of Each Epoch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, test_func, text_img_gen,is_train,acc_compute_batches):\n",
    "        self.test_func = test_func\n",
    "        self.text_img_gen = text_img_gen\n",
    "        self.is_train=is_train                #used to indicate whether the callback is called to for Train or Validation Data\n",
    "        self.acc_batches=acc_compute_batches  # Number of Batches for which the metrics are computed typically equal to steps/epoch\n",
    "\n",
    "    def show_accuracy_metrics(self,num_batches):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy and letter accuracy for each batch of inputs, \n",
    "        and prints the avarage accuracy and letter accuracy across all the batches\n",
    "        \"\"\"\n",
    "        accuracy=0\n",
    "        letter_accuracy=0\n",
    "        batches_cnt=num_batches\n",
    "        while batches_cnt>0:\n",
    "            word_batch = next(self.text_img_gen)[0]   #Gets the next batch from the Data generator\n",
    "            decoded_res = decode_batch(self.test_func,word_batch['img_input'])\n",
    "            actual_res=word_batch['source_str']\n",
    "            acc,let_acc=accuracies(actual_res,decoded_res,self.is_train)\n",
    "            accuracy+=acc\n",
    "            letter_accuracy+=let_acc\n",
    "            batches_cnt-=1\n",
    "        accuracy=accuracy/num_batches\n",
    "        letter_accuracy=letter_accuracy/num_batches\n",
    "        if self.is_train:\n",
    "            print(\"Train Average Accuracy of \"+str(num_batches)+\" Batches: \",np.round(accuracy,2),\" %\")\n",
    "            print(\"Train Average Letter Accuracy of \"+str(num_batches)+\" Batches: \",np.round(letter_accuracy,2),\" %\")\n",
    "        else:\n",
    "            print(\"Validation Average Accuracy of \"+str(num_batches)+\" Batches: \",np.round(accuracy,2),\" %\")\n",
    "            print(\"Validation Average Letter Accuracy of \"+str(num_batches)+\" Batches: \",np.round(letter_accuracy,2),\" %\")\n",
    "            \n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.show_accuracy_metrics(self.acc_batches)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# early_stop=EarlyStopping(monitor='val_loss',patience=2,restore_best_weights=True)\n",
    "# model_chk_pt=ModelCheckpoint('weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', save_best_only=False,save_weights_only=True,verbose=0, mode='auto', period=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logdir = os.path.join(\"logs_127\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs_127"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Labels Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Train Data Labels\n",
    "Train_labels=[str(x) for x in train_df['text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_paths=[str(x) for x in train_data['ImageName'].values]\n",
    "# import os\n",
    "\n",
    "img_path = 'EvArEST/Arabic_words_train/'\n",
    "image_names = train_df['image']\n",
    "\n",
    "train_paths = [os.path.join(img_path, name) for name in image_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nan_cnt=0\n",
    "train_nan_replaced=False\n",
    "for i in range(len(Train_labels)):\n",
    "    if Train_labels[i]=='nan':\n",
    "        Train_labels[i]='NULL'\n",
    "        train_nan_replaced=True\n",
    "        train_nan_cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was there any NULL values written as Nan in Train Data: False\n",
      "Train Nan count:  0\n"
     ]
    }
   ],
   "source": [
    "print('Was there any NULL values written as Nan in Train Data:',train_nan_replaced)\n",
    "print('Train Nan count: ',train_nan_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instatiating Data Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gene=DataGenerator(train_paths,img_w, img_h,batch_size,len(Train_labels),Train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4187  Image Loading start...\n",
      "Loaded Images:  0\n",
      "Loaded Images:  1000\n",
      "Loaded Images:  2000\n",
      "Loaded Images:  3000\n",
      "Loaded Images:  4000\n",
      "Number of Texts matches with Total Number of Images : True\n",
      "4187  Image Loading finish...\n"
     ]
    }
   ],
   "source": [
    "train_gene.build_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display two random images from the dataset along with their label\n",
    "\n",
    "# # Select two random indices\n",
    "# indices = random.sample(range(len(train_gene.imgs)), 2)\n",
    "\n",
    "# # Display the images and their corresponding labels\n",
    "# for i in range(len(indices)):\n",
    "#     index = indices[i]\n",
    "#     img = train_gene.imgs[index]\n",
    "#     label=encode_words_labels(train_gene.texts[index])\n",
    "#     word = words_from_labels(label)\n",
    "#     plt.subplot(1, 2, i+1)\n",
    "#     plt.imshow(img, cmap='gray')\n",
    "#     reshaped_text = arabic_reshaper.reshape(word) #Arabic reshaper for Arabic language\n",
    "#     display_text = get_display(reshaped_text)\n",
    "#     plt.title(display_text) #Display the title in Arabic format\n",
    "#     plt.axis('off')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz_cb_train = VizCallback( test_func, train_gene.next_batch(),True,train_num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "adam=optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Keras functionnal API, we can define, train and use a neural network using the Model class. The loss functions that can be used in a class Model have only 2 arguments, the ground truth y_true and the prediction y_pred given in output of the neural network. At present, there is no CTC loss proposed in a Keras Model and, Keras doesn’t currently support loss functions with extra parameters, which is the case for a CTC loss that requires sequence lengths for batch training. Indeed, as observation and label sequences are of variable length, one has to provide the length of each sequence (observation and label), which allows to not consider padding in the loss computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Dummy Loss function as in Keras there is no CTC loss implementation which actually takes 4 inputs \n",
    "#The loss function in keras accepts only 2 inputs, so create a dummy loss which is a work around for implementing CTC in Keras\n",
    "#The Actual loss computation happens in ctc_loss_function defined above\n",
    "img_text_recog.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.1. Train Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "65/65 [==============================] - ETA: 0s - loss: 20.9169"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Failed to format this callback filepath: \"best_model.{epoch:02d}-{val_loss:.2f}.h5\". Reason: \\'val_loss\\''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-bb059bd52cc5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m ]\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m img_text_recog.fit_generator(generator=train_gene.next_batch(),\n\u001b[0m\u001b[0;32m      7\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gene\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ai702\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2258\u001b[0m         \u001b[1;34m'Please use `Model.fit`, which supports generators.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2259\u001b[0m         stacklevel=2)\n\u001b[1;32m-> 2260\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   2261\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2262\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ai702\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ai702\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, batch, logs)\u001b[0m\n\u001b[0;32m   1485\u001b[0m             epoch=epoch + 1, batch=batch + 1, **logs)\n\u001b[0;32m   1486\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1487\u001b[1;33m       raise KeyError(\n\u001b[0m\u001b[0;32m   1488\u001b[0m           \u001b[1;34mf'Failed to format this callback filepath: \"{self.filepath}\". '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1489\u001b[0m           f'Reason: {e}')\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Failed to format this callback filepath: \"best_model.{epoch:02d}-{val_loss:.2f}.h5\". Reason: \\'val_loss\\''"
     ]
    }
   ],
   "source": [
    "callbacks_list = [keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "        monitor='val_accuracy', save_best_only=True), keras.callbacks.EarlyStopping(monitor='val_loss', patience=3), train_gene\n",
    "]\n",
    "\n",
    "img_text_recog.fit_generator(generator=train_gene.next_batch(),\n",
    "                    steps_per_epoch=int(train_gene.n / batch_size),\n",
    "                    epochs=20,\n",
    "                    callbacks=callbacks_list,)\n",
    "#callbacks=[viz_cb_train,viz_cb_val,train_gene,val_gen,tensorboard_callback,early_stop,model_chk_pt],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_text_recog.save('Best_Img_recog_LSTM_Adam_model_run_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_text_recog.save('Img_recog_LSTM_Adam_model_run_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train CTC Loss Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('LSTM_Model_Train_Loss_Plot.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation CTC Loss Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('LSTM_Model_Val_loss_Plot.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2. Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model with Bi-Directional GRU units and RAdam Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since GRU is also RNN with faster training time than LSTM, Creating the same architecture with Bi-directional GRU units and RAdam Optimizer to observe if this model architecture gives better results than the previous model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Model Architecture has 2 Stages:\n",
    "1. **Train Stage:** which takes input image, text labels (encoded as integers), input length (time-steps length), label length and outputs CTC loss\n",
    "2. **Prediction Stage:** which takes input image and outputs a matrix of dimesnions 48x37 where 48 is the number of time-steps of RNN and 37 is the length of letters + 1 character for ctc blank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Image_text_recogniser_model_2(stage,drop_out_rate=0.35):\n",
    "    \"\"\"\n",
    "    Builds the model by taking in the stage variable which specifes the stage,\n",
    "    if the stage is training: model takes inputs required for computing ctc_batch_cost function\n",
    "    else : model takes input as images which is used for prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (1, img_w, img_h)\n",
    "    else:\n",
    "        input_shape = (img_w, img_h, 1)\n",
    "       \n",
    "    model_input=Input(shape=input_shape,name='img_input',dtype='float32')\n",
    "\n",
    "    # Convolution layer \n",
    "    model = Conv2D(64, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(model_input) \n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = MaxPooling2D(pool_size=(2, 2), name='max1')(model) \n",
    "\n",
    "    model = Conv2D(128, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(model) \n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = MaxPooling2D(pool_size=(2, 2), name='max2')(model) \n",
    "\n",
    "    model = Conv2D(256, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(model) \n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(256, (3, 3), padding='same', name='conv4', kernel_initializer='he_normal')(model)\n",
    "    model=Dropout(drop_out_rate)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = MaxPooling2D(pool_size=(1, 2), name='max3')(model)  \n",
    "\n",
    "    model = Conv2D(512, (3, 3), padding='same', name='conv5', kernel_initializer='he_normal')(model) \n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = Conv2D(512, (3, 3), padding='same', name='conv6')(model)\n",
    "    model=Dropout(drop_out_rate)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "    model = MaxPooling2D(pool_size=(1, 2), name='max4')(model) \n",
    "\n",
    "    model = Conv2D(512, (2, 2), padding='same', kernel_initializer='he_normal', name='con7')(model)\n",
    "    model=Dropout(0.25)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    model = Activation('relu')(model)\n",
    "#     print(model.shape)\n",
    "    \n",
    "\n",
    "    # CNN to RNN\n",
    "    model = Reshape(target_shape=((42, 1024)), name='reshape')(model)  \n",
    "    model = Dense(64, activation='relu', kernel_initializer='he_normal', name='dense1')(model)  \n",
    "\n",
    "    # RNN layer\n",
    "    model=Bidirectional(GRU(256, return_sequences=True, kernel_initializer='he_normal'), merge_mode='sum')(model)\n",
    "    model=Bidirectional(GRU(256, return_sequences=True, kernel_initializer='he_normal'), merge_mode='concat')(model)\n",
    "\n",
    "    # transforms RNN output to character activations:\n",
    "    model = Dense(num_classes, kernel_initializer='he_normal',name='dense2')(model) \n",
    "    y_pred = Activation('softmax', name='softmax')(model)\n",
    "\n",
    "    \n",
    "    labels = Input(name='ground_truth_labels', shape=[max_length], dtype='float32') \n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64') \n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64') \n",
    "\n",
    "    #CTC loss function\n",
    "    loss_out = Lambda(ctc_loss_function, output_shape=(1,),name='ctc')([y_pred, labels, input_length, label_length]) #(None, 1)\n",
    "\n",
    "    if stage=='train':\n",
    "        return model_input,y_pred,Model(inputs=[model_input, labels, input_length, label_length], outputs=loss_out)\n",
    "    else:\n",
    "        return Model(inputs=[model_input], outputs=y_pred)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input,y_pred,img_text_recog=Image_text_recogniser_model_2('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #used for visualization\n",
    "test_func = K.function([model_input], [y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_text_recog.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_cb_train = VizCallback( test_func, train_gene.next_batch(),True,train_num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_cb_val = VizCallback( test_func, val_gen.next_batch(),False,val_num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining RAdam Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Reading Articles on Medium Blog, I came across an article on a New State of the Art Optimizer, Rectified Adam (RAdam). I wanted to try it out and see the results compared to Adam optimizer. So I tried RAdam optimizer with exact same model architecture as the one used for Adam Optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pypi.org/project/keras-radam/\n",
    "from keras_radam import RAdam\n",
    "Radam=RAdam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Keras functionnal API, we can define, train and use a neural network using the Model class. The loss functions that can be used in a class Model have only 2 arguments, the ground truth y_true and the prediction y_pred given in output of the neural network. At present, there is no CTC loss proposed in a Keras Model and, Keras doesn’t currently support loss functions with extra parameters, which is the case for a CTC loss that requires sequence lengths for batch training. Indeed, as observation and label sequences are of variable length, one has to provide the length of each sequence (observation and label), which allows to not consider padding in the loss computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Dummy Loss function as in Keras there is no CTC loss implementation which actually takes 4 inputs \n",
    "#The loss function in keras accepts only 2 inputs, so create a dummy loss which is a work around for implementing CTC in Keras\n",
    "#The Actual loss computation happens in ctc_loss_function defined above\n",
    "img_text_recog.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=Radam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.1. Train Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_text_recog.fit_generator(generator=train_gene.next_batch(),\n",
    "                    steps_per_epoch=int(train_gene.n / batch_size),\n",
    "                    epochs=20,\n",
    "                    callbacks=[viz_cb_train,viz_cb_val,train_gene,val_gen,tensorboard_callback,early_stop,model_chk_pt],\n",
    "                    validation_data=val_gen.next_batch(),\n",
    "                    validation_steps=int(val_gen.n / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_text_recog.save('Best_Img_recog_GRU_RAdam_model_run_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 2 Train CTC Loss Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('GRU_Model_train_Loss_Plot.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 2 Validation CTC Loss Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('GRU_Model_Val_Loss_Plot.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Training Time for Model 2 lower than Model 1 \n",
    "2. Train and Validation CTC loss for Model 1 is comparatively lower than the Train, Validation CTC loss for Model 2 after 50 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Model Output Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1. Best Path Decoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Function Takes the final 40x37 output matrix from the model, and takes the argmax of the matrix across each column (which returns a value between 0 to 36 (ctc_blank) both included). The outputs are then merged for repeated values and gives a list of integers. The Final output integers is then converted to final output string text and it is returned by the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://keras.io/examples/image_ocr/\n",
    "#https://github.com/qjadud1994/CRNN-Keras\n",
    "def decode_label(out):\n",
    "    \"\"\"\n",
    "    Takes the predicted ouput matrix from the Model and returns the output text for the image\n",
    "    \"\"\"\n",
    "    # out : (1, 42, 37)\n",
    "    # discarding first 2 outputs of RNN as they tend to be garbage \n",
    "    out_best = list(np.argmax(out[0,2:], axis=1))\n",
    "\n",
    "    out_best = [k for k, g in itertools.groupby(out_best)]  # remove overlap value\n",
    "\n",
    "    outstr=words_from_labels(out_best)\n",
    "    return outstr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2. Test Output Prediction Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Function Takes in Following Inputs:\n",
    "1. **model:** The Best Model obtained after training\n",
    "2. **test_img_names:** The Image folder path where the input image is located\n",
    "3. **test_labels:** The Ground Truth Text Labels of Corresponding input Images\n",
    "\n",
    "<p> The function reads the images in the folder path and  resizes it to 32x100 dimension,single channel gray scale image.The processed image is passed to to best model which predicts the output which is a 1x48x37 matrix. The output Matrix is passed to CTC Best path decoding function which returns the predicted text word. The function then compares this predicted word text with the expected Ground truth Label and calculates Accuracy and Letter-Accuracy across the entire test data and returns accuracy, letter accuracy, letter count and letter mis match count.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_output_Prediction(model,test_img_names,test_labels):\n",
    "    \"\"\"\n",
    "    Takes the best model, test data image paths, test data groud truth labels and pre-processes the input image to \n",
    "    appropriate format for the model prediction, takes the predicted output matrix and uses best path decoding to \n",
    "    generate predicted text and compares with ground truth text for the input and ouputs the final accuracy,\n",
    "    letter accuracy and  letter count across the entire test set of images, it also returns list of letter mis-match\n",
    "    count for each test point in the whole test set of images\n",
    "    \"\"\"\n",
    "    start=datetime.now()\n",
    "    accuracy=0\n",
    "    letter_acc=0\n",
    "    letter_cnt=0\n",
    "    count=0\n",
    "    letter_mis_match=[]\n",
    "    for i in range(len(test_labels)):\n",
    "        test_img=cv2.imread(test_img_names[i])\n",
    "        test_img_resized=cv2.resize(test_img,(170,32))\n",
    "        test_image=test_img_resized[:,:,1]\n",
    "        test_image=test_image.T\n",
    "        test_image=np.expand_dims(test_image,axis=-1)\n",
    "        test_image=np.expand_dims(test_image, axis=0)\n",
    "        test_image=test_image/255\n",
    "        model_output=model.predict(test_image)\n",
    "        predicted_output=decode_label(model_output)\n",
    "        actual_output=test_labels[i]\n",
    "        count+=1\n",
    "        mis_match=0\n",
    "        for j in range(min(len(predicted_output),len(actual_output))):\n",
    "            if predicted_output[j]==actual_output[j]:\n",
    "                letter_acc+=1\n",
    "            else:\n",
    "                mis_match+=1\n",
    "        letter_cnt+=max(len(predicted_output),len(actual_output))\n",
    "        letter_mis_match.append(mis_match)\n",
    "        if actual_output==predicted_output:\n",
    "            accuracy+=1\n",
    "        if (count%1000)==0:\n",
    "            print(\"Processed \",count,\" Images\")\n",
    "    print(\"Time Taken for Processing: \",datetime.now()-start)\n",
    "    return accuracy,letter_acc,letter_cnt,letter_mis_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3. Model 1 Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Output Prediction, load the model achitecture  defined above for Predict stage which takes in the image as input and outputs a 48x37 matrix as ouput for each input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Image_text_recogniser_model_1('predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Best Weights for the Model are stored in BestLSTMModelWeights Folder, loading the stored best weights for Model 1 for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('Final_LSTM_Model_Best_Weights/Best_Img_recog_LSTM_Adam_model_run_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Synth Text Validation Data Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_names=val_data['ImageName'].values\n",
    "val_labels=val_data['Labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_val_accuracy,synth_val_letter_acc,synth_val_letter_cnt,synth_val_mis_match=test_data_output_Prediction(model,val_img_names,val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Output Accuracy: \",(synth_val_accuracy/len(val_labels))*100, \" %\")\n",
    "print(\"Model Output Letter Accuracy: \",(synth_val_letter_acc/synth_val_letter_cnt)*100, \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_val_mis_match_dict=Counter(synth_val_mis_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 1 Validation Data Prediction Analysis upto 4 Character Mis-Matches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_match_cnts_1=[]\n",
    "for i in range(5):\n",
    "    mis_match_cnts_1.append(model_1_val_mis_match_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mis_match_character_analysis_plot(mis_match_counts,num_values):\n",
    "    \"\"\"\n",
    "    Takes mis-match counts upto 4 characters of the predicted output, total number of values and\n",
    "    plots the percentage of number of mis-match characters between predicted and actual labels\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,6))\n",
    "    indices=np.arange(len(mis_match_counts))\n",
    "    counts=np.array(mis_match_counts)\n",
    "    percent=(counts/num_values)*100\n",
    "    plt.bar(indices,percent)\n",
    "    plt.xlabel('Number of Mis-Match Characters',fontsize=10)\n",
    "    plt.ylabel('Percentages',fontsize=10)\n",
    "    plt.title('Percentages of Number of Mis-Match Characters',fontsize=12)\n",
    "    plt.xticks(indices,indices)\n",
    "    plt.show()\n",
    "    for i in range(len(indices)):\n",
    "        print(i,\" Mis-Match Characters Percentage: \",np.round(percent[i],2),\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_match_character_analysis_plot(mis_match_cnts_1,12000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Synth Text Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_names=test_data['ImageName'].values\n",
    "test_labels=test_data['Labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_test_accuracy,synth_test_letter_acc,synth_test_letter_cnt,synth_test_mis_match=test_data_output_Prediction(model,test_img_names,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Output Accuracy: \",(synth_test_accuracy/len(test_labels))*100, \" %\")\n",
    "print(\"Model Output Letter Accuracy: \",(synth_test_letter_acc/synth_test_letter_cnt)*100, \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_test_mis_match_dict=Counter(synth_test_mis_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 1 Test Data Prediction Analysis upto 4 Character Mis-Matches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_match_cnts_2=[]\n",
    "for i in range(5):\n",
    "    mis_match_cnts_2.append(model_1_test_mis_match_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_match_character_analysis_plot(mis_match_cnts_2,15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.4. Model 2 Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Output Prediction, load the model achitecture  defined above for Predict stage which takes in the image as input and outputs a 48x37 matrix as ouput for each input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2=Image_text_recogniser_model_2('predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Best Weights for the Model are stored in BestGRUModelWeights Folder, loading the stored best weights for Model 2 for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.load_weights('Final_GRU_Model_Best_Weights/Best_Img_recog_GRU_RAdam_model_run_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Synth Text Validation Data Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_names=val_data['ImageName'].values\n",
    "val_labels=val_data['Labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_val_accuracy,synth_val_letter_acc,synth_val_letter_cnt,synth_val_mis_match=test_data_output_Prediction(model_2,val_img_names,val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Output Accuracy: \",(synth_val_accuracy/len(val_labels))*100, \" %\")\n",
    "print(\"Model Output Letter Accuracy: \",(synth_val_letter_acc/synth_val_letter_cnt)*100, \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_val_mis_match_dict=Counter(synth_val_mis_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 2 Validation Data Prediction Analysis upto 4 Character Mis-Matches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_match_cnts_3=[]\n",
    "for i in range(5):\n",
    "    mis_match_cnts_3.append(model_2_val_mis_match_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_match_character_analysis_plot(mis_match_cnts_3,12000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Synth Text Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_names=test_data['ImageName'].values\n",
    "test_labels=test_data['Labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_test_accuracy,synth_test_letter_acc,synth_test_letter_cnt,synth_test_mis_match=test_data_output_Prediction(model_2,test_img_names,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Output Accuracy: \",(synth_test_accuracy/len(test_labels))*100, \" %\")\n",
    "print(\"Model Output Letter Accuracy: \",(synth_test_letter_acc/synth_test_letter_cnt)*100, \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_test_mis_match_dict=Counter(synth_test_mis_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 2 Test Data Prediction Analysis upto 4 Character Mis-Matches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_match_cnts_4=[]\n",
    "for i in range(5):\n",
    "    mis_match_cnts_4.append(model_2_test_mis_match_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_match_character_analysis_plot(mis_match_cnts_4,15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Model 1:** Model 1 Architecture consists of Convulution Layers, LSTM Units for RNN and uses Adam Optimizer\n",
    "2. **Model 2:** Model 2 Architecture consists of Convulution Layers, GRU Units for RNN and uses RAdam Optimizer\n",
    "3. **Val:** Denotes Synth Text Validation Data containing 12000 Images\n",
    "4. **Test:** Denotes Synth Text Test Data containing 15000 Images\n",
    "5. **0 MM(%):** Denotes % of points out of total points which have 0 character mis-match between Predicted and Actual Labels\n",
    "6. **1 MM(%):** Denotes % of points out of total points which have 1 character mis-match between Predicted and Actual Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "pt = PrettyTable()\n",
    "\n",
    "pt.field_names = [\"Model\", \"Data\", \"Accuracy\",\"Letter Accuracy\", \"0 MM(%)\", \"1 MM (%)\"]\n",
    "\n",
    "pt.add_row([\"Model 1\", \"Val\", 86.47, 94.01, 86.58, 5.78])\n",
    "\n",
    "pt.add_row([\"Model 1\", \"Test\", 87.98, 94.48, 87.19, 5.85])\n",
    "\n",
    "pt.add_row([\"Model 2\", \"Val\", 81.86, 92.1, 82.06, 7.32])\n",
    "\n",
    "pt.add_row([\"Model 2\", \"Test\", 82.43, 92.63, 82.73, 7.45])\n",
    "\n",
    "print(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The 2 Image Text Recognition Model Architectures are Trained on 200000 Synth Text Images of Variable length Labels which are different from Validation Data containing 12000 images and Test Data Containing 15000 images.\n",
    "2. Model 1 Trained on 200000 images from Synth Text Images performs reasonably well on Unseen 15000 Test Images of Variable length labels with an accuracy of ~88% and letter accuracy of ~94%\n",
    "3. Model 2 also Trained on same 200000 images has an accuracy of ~82% and letter accuracy of ~93% on 15000 Test Images of Variable length labels\n",
    "4. Both the Models have high percentage (>82%) of 0 mis-match character points between Actual and Predicted Labels "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ai702]",
   "language": "python",
   "name": "conda-env-ai702-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
